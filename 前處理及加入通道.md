# YOLO 前處理及多通道輸入實作指南

## 概述
本文件詳細說明如何在 Ultralytics YOLO 中實作 CLAHE 和 Canny 前處理，並透過增加通道數從 RGB (3通道) 擴展到 4通道 (RGB + Enhanced Canny) 來改善模型效能。

## 技術架構

### 1. 多通道輸入設計
- **原始輸入**: RGB 3通道
- **擴展輸入**: RGB (3通道) + Enhanced Canny (1通道) = 4通道
- **Enhanced Canny 處理流程**: 原圖 → Grayscale → CLAHE → Canny → 邊緣特徵通道
- **優勢**: 保留原始色彩信息的同時，增加經過對比度增強的高品質邊緣特徵

### 2. 前處理技術說明

#### CLAHE (對比度限制自適應直方圖均衡化)
- **目的**: 增強局部對比度，改善低對比度區域的可見度
- **參數**:
  - `clipLimit`: 2.0-4.0 (避免過度增強)
  - `tileGridSize`: (8,8) 或 (16,16) (根據圖像尺寸調整)
- **應用場景**: 低光環境、醫學影像、衛星圖像

#### Canny 邊緣檢測
- **目的**: 提供清晰的邊緣信息，幫助模型更好地識別物體邊界
- **參數**:
  - `lower_threshold`: 50-100
  - `upper_threshold`: 150-200
  - `aperture_size`: 3 或 5
- **應用場景**: 邊界模糊的物體、精密檢測任務

#### Enhanced Canny 處理流程
- **處理順序**: 
  1. **Grayscale**: 將 RGB 轉換為灰度圖
  2. **CLAHE**: 對灰度圖進行對比度限制自適應直方圖均衡化
  3. **Canny**: 對 CLAHE 處理後的圖像進行邊緣檢測
- **最終輸出**: 高品質的邊緣特徵圖作為第4個通道
- **優勢**: 
  - CLAHE 增強對比度，使邊緣更加清晰
  - Canny 檢測出的邊緣更加準確和完整
  - 對低對比度區域的邊緣檢測效果顯著提升

## 實作方案

### 方案一：修改資料讀取層 (推薦)

#### 1. 修改 BaseDataset.load_image 方法
**檔案**: `ultralytics/data/base.py`
**修改位置**: `load_image` 方法 (第202行)

```python
def load_image(self, i, rect_mode=True):
    """Load an image from dataset index 'i' with enhanced preprocessing."""
    im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]
    if im is None:  # not cached in RAM
        if fn.exists():  # load npy
            try:
                im = np.load(fn)
                # 檢查是否為4通道緩存
                if im.shape[2] != 4:
                    im = self._apply_multi_channel_preprocessing(im)
            except Exception as e:
                LOGGER.warning(f"{self.prefix}Removing corrupt *.npy image file {fn} due to: {e}")
                Path(fn).unlink(missing_ok=True)
                im = imread(f)  # BGR
                im = self._apply_multi_channel_preprocessing(im)
        else:  # read image
            im = imread(f)  # BGR
            im = self._apply_multi_channel_preprocessing(im)
        
        if im is None:
            raise FileNotFoundError(f"Image Not Found {f}")

        h0, w0 = im.shape[:2]  # orig hw
        if rect_mode:  # resize long side to imgsz while maintaining aspect ratio
            r = self.imgsz / max(h0, w0)  # ratio
            if r != 1:  # if sizes are not equal
                w, h = (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz))
                im = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)
        elif not (h0 == w0 == self.imgsz):  # resize by stretching image to square imgsz
            im = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)

        # Add to buffer if training with augmentations
        if self.augment:
            self.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized
            self.buffer.append(i)
            if 1 < len(self.buffer) >= self.max_buffer_length:  # prevent empty buffer
                j = self.buffer.pop(0)
                if self.cache != "ram":
                    self.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None

        return im, (h0, w0), im.shape[:2]

    return self.ims[i], self.im_hw0[i], self.im_hw[i]

def _apply_multi_channel_preprocessing(self, im):
    """Apply enhanced canny preprocessing to create 4-channel input."""
    if im.shape[2] == 3:  # RGB input
        # 步驟1: 轉換為灰度圖
        gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
        
        # 步驟2: CLAHE 處理灰度圖
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        clahe_enhanced = clahe.apply(gray)
        
        # 步驟3: 對 CLAHE 處理後的圖像進行 Canny 邊緣檢測
        enhanced_canny = cv2.Canny(clahe_enhanced, 50, 150, apertureSize=3)
        
        # 組合為4通道: RGB + Enhanced Canny
        multi_channel = np.dstack([
            im,  # 原始 BGR 3通道
            enhanced_canny  # Enhanced Canny 1通道
        ])
        
        return multi_channel
    
    return im  # 已經是4通道，直接返回
```

#### 2. 修改模型定義
**檔案**: `ultralytics/nn/tasks.py`
**修改位置**: DetectionModel 類的 `__init__` 方法 (第311行)

```python
def __init__(self, cfg="yolo11n.yaml", ch=4, nc=None, verbose=True):  # 將 ch=3 改為 ch=4
    """
    Initialize the YOLO detection model with the given config and parameters.

    Args:
        cfg (str | dict): Model configuration file path or dictionary.
        ch (int): Number of input channels. Default is 4 for RGB+Enhanced Canny.
        nc (int, optional): Number of classes. If None, it will be inferred from the config.
        verbose (bool): Whether to print verbose output during model construction.
    """
    super().__init__(cfg, ch=ch, verbose=verbose)
    self.yaml = cfg if isinstance(cfg, dict) else yaml_load(check_yaml(cfg), append_filename=True)
    
    # Define model
    ch = self.yaml.get('ch', ch)  # input channels
    # ... rest of the method remains the same
```

#### 3. 創建自定義配置檔案
**檔案**: `ultralytics/cfg/models/v8/yolov8-4ch.yaml`

```yaml
# Ultralytics YOLOv8 4-channel (RGB+Enhanced Canny) object detection model
# Based on YOLOv8 with modified input channels

# Parameters
nc: 80 # number of classes
ch: 4  # input channels (RGB + Enhanced Canny)
scales: # model compound scaling constants
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024] # YOLOv8n summary: 129 layers
  s: [0.33, 0.50, 1024] # YOLOv8s summary: 129 layers  
  m: [0.67, 0.75, 768]  # YOLOv8m summary: 169 layers
  l: [1.00, 1.00, 512]  # YOLOv8l summary: 209 layers
  x: [1.00, 1.25, 512]  # YOLOv8x summary: 209 layers

# YOLOv8 backbone with 4-channel input
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - First conv layer handles 4 channels
  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]] # 9

# YOLOv8 head (unchanged)
head:
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 6], 1, Concat, [1]] # cat backbone P4
  - [-1, 3, C2f, [512]] # 12

  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 4], 1, Concat, [1]] # cat backbone P3
  - [-1, 3, C2f, [256]] # 15 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]] # cat head P4
  - [-1, 3, C2f, [512]] # 18 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]] # cat head P5
  - [-1, 3, C2f, [1024]] # 21 (P5/32-large)

  - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)
```

### 方案二：自定義變換層 (替代方案)

如果不想修改核心代碼，可以創建自定義的變換類：

**檔案**: `ultralytics/data/augment.py` (添加到文件末尾)

```python
class MultiChannelPreprocess:
    """Apply CLAHE and Canny preprocessing to create multi-channel input."""
    
    def __init__(self, clahe_clip_limit=3.0, clahe_tile_grid=(8,8), 
                 canny_low=50, canny_high=150, canny_aperture=3):
        """
        Initialize multi-channel preprocessing.
        
        Args:
            clahe_clip_limit (float): CLAHE clip limit
            clahe_tile_grid (tuple): CLAHE tile grid size
            canny_low (int): Canny lower threshold
            canny_high (int): Canny upper threshold
            canny_aperture (int): Canny aperture size
        """
        self.clahe_clip_limit = clahe_clip_limit
        self.clahe_tile_grid = clahe_tile_grid
        self.canny_low = canny_low
        self.canny_high = canny_high
        self.canny_aperture = canny_aperture
        
    def __call__(self, labels):
        """Apply multi-channel preprocessing to labels."""
        im = labels['img']
        
        if im.shape[2] == 3:  # RGB input
            # 步驟1: 轉換為灰度圖
            gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
            
            # 步驟2: CLAHE 處理灰度圖
            clahe = cv2.createCLAHE(clipLimit=self.clahe_clip_limit, 
                                   tileGridSize=self.clahe_tile_grid)
            clahe_enhanced = clahe.apply(gray)
            
            # 步驟3: 對 CLAHE 處理後的圖像進行 Canny 邊緣檢測
            enhanced_canny = cv2.Canny(clahe_enhanced, self.canny_low, self.canny_high, 
                                      apertureSize=self.canny_aperture)
            
            # 組合為4通道: RGB + Enhanced Canny
            multi_channel = np.dstack([im, enhanced_canny])
            labels['img'] = multi_channel
            
        return labels
```

## 使用說明

### 1. 訓練
```python
from ultralytics import YOLO

# 使用4通道模型配置
model = YOLO('yolov8n-4ch.yaml')

# 訓練模型
results = model.train(
    data='coco8.yaml',
    epochs=100,
    imgsz=640,
    batch=16
)
```

### 2. 驗證
```python
# 驗證模型
results = model.val(data='coco8.yaml')
```

### 3. 預測
```python
# 預測 (系統會自動應用前處理)
results = model.predict('path/to/images')
```

### 4. 導出
```python
# 導出為 ONNX (需要指定輸入形狀)
model.export(format='onnx', imgsz=640)
```

## 性能考量

### 1. 計算開銷
- **CLAHE**: 輕量級，對性能影響小
- **Canny**: 中等開銷，但提供有價值的邊緣信息
- **總體**: 預處理時間增加約 15-25%

### 2. 記憶體使用
- **原始**: 3通道 → 新架構: 4通道
- **記憶體增加**: 約 33% (4/3 = 1.33)
- **建議**: 考慮降低批次大小 15-20%

### 3. 模型大小
- **參數增加**: 第一層卷積參數從 3×64 增加到 4×64
- **總體影響**: 模型大小增加 < 1% (僅第一層受影響)

## 最佳化建議

### 1. 參數調優
```python
# CLAHE 參數建議
clahe_configs = {
    'indoor': {'clipLimit': 2.0, 'tileGridSize': (8,8)},
    'outdoor': {'clipLimit': 3.0, 'tileGridSize': (16,16)},
    'medical': {'clipLimit': 4.0, 'tileGridSize': (4,4)}
}

# Canny 參數建議
canny_configs = {
    'general': {'low': 50, 'high': 150},
    'fine_detail': {'low': 30, 'high': 100},
    'coarse_edges': {'low': 100, 'high': 200}
}
```

### 2. 緩存策略
- 啟用磁盤緩存以避免重複前處理
- 考慮使用 RAM 緩存加速訓練 (如果記憶體充足)

### 3. 數據增強調整
```python
# 調整數據增強以適應多通道輸入
augmentation_config = {
    'hsv_h': 0.015,      # 降低色彩變化 (保護CLAHE效果)
    'hsv_s': 0.7,        
    'hsv_v': 0.4,
    'degrees': 10,       # 降低旋轉角度 (保護邊緣信息)
    'shear': 0.0,        # 禁用剪切 (避免邊緣失真)
}
```

## 常見問題與解決方案

### 1. Q: 預訓練權重不兼容
**A**: 需要進行權重遷移或從頭訓練
```python
# 權重遷移示例
def transfer_weights(old_model_path, new_model):
    old_weights = torch.load(old_model_path)
    new_weights = new_model.state_dict()
    
    # 跳過第一層卷積
    for key, value in old_weights.items():
        if 'model.0.conv.weight' not in key:
            if key in new_weights:
                new_weights[key] = value
    
    # 手動初始化第一層 (前3個通道使用預訓練權重)
    if 'model.0.conv.weight' in old_weights:
        old_conv = old_weights['model.0.conv.weight']  # shape: [64, 3, 3, 3]
        new_conv = torch.zeros(64, 4, 3, 3)
        new_conv[:, :3, :, :] = old_conv
        # 第4個通道使用邊緣檢測相關的權重初始化
        # 使用 Sobel 類似的邊緣檢測權重模式
        edge_weights = torch.zeros_like(new_conv[:, 3:4, :, :])
        # 初始化為小的隨機值，適合邊緣特徵
        torch.nn.init.xavier_uniform_(edge_weights, gain=0.1)
        new_conv[:, 3:4, :, :] = edge_weights
        new_weights['model.0.conv.weight'] = new_conv
    
    new_model.load_state_dict(new_weights)
```

### 2. Q: 批次大小需要調整
**A**: 建議將批次大小減少 15-25%

### 3. Q: 某些應用場景效果不佳
**A**: 
- 對於高對比度圖像，考慮跳過 CLAHE
- 對於紋理豐富的圖像，調低 Canny 閾值
- 使用場景特定的參數配置

## 評估指標

### 1. 性能指標
- **mAP**: 期望提升 2-8% (取決於數據集)
- **精確率**: 邊緣清晰物體的檢測精確率顯著提升
- **召回率**: 低對比度環境下的召回率改善

### 2. 效率指標
- **推理速度**: 下降 10-15% (前處理開銷)
- **訓練速度**: 下降 15-25%
- **記憶體使用**: 增加 30-40%

## 進階優化

### 1. 動態前處理
```python
class AdaptivePreprocessing:
    """根據圖像特性動態調整前處理參數"""
    
    def __init__(self):
        self.clahe = cv2.createCLAHE()
        
    def analyze_image(self, img):
        """分析圖像特性"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        contrast = np.std(gray)
        brightness = np.mean(gray)
        return contrast, brightness
    
    def adaptive_clahe(self, img, contrast, brightness):
        """自適應 CLAHE 參數"""
        if contrast < 30:  # 低對比度
            clip_limit = 4.0
            tile_size = (4, 4)
        elif contrast > 80:  # 高對比度
            clip_limit = 1.5
            tile_size = (16, 16)
        else:  # 中等對比度
            clip_limit = 2.5
            tile_size = (8, 8)
            
        self.clahe.setClipLimit(clip_limit)
        self.clahe.setTilesGridSize(tile_size)
        return self.clahe.apply(img)
```

### 2. 並行處理
```python
import concurrent.futures

def parallel_preprocess(images):
    """並行處理多張圖像"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(apply_preprocessing, img) for img in images]
        results = [future.result() for future in concurrent.futures.as_completed(futures)]
    return results
```

## 總結

本實作方案提供了在 YOLO 中集成 CLAHE 和 Canny 前處理的完整解決方案。通過循序漸進的前處理流程 (原圖 → Grayscale → CLAHE → Canny)，產生高品質的邊緣特徵作為第4個通道，模型可以同時利用原始色彩信息和經過對比度增強的精確邊緣特徵。

**建議的實作順序**:
1. 實作方案一中的核心修改
2. 創建4通道模型配置檔案  
3. 調整 CLAHE 和 Canny 參數以優化邊緣檢測品質
4. 進行小規模測試和參數調優
5. 擴展到完整數據集訓練
6. 根據具體應用場景優化前處理參數

**預期收益**:
- 低光環境檢測性能提升 8-20% (CLAHE 增強對比度效果)
- 邊界模糊物體檢測精確度提升 10-25% (Enhanced Canny 邊緣檢測)
- 整體 mAP 提升 3-10%
- 記憶體開銷適中 (增加33%)

**參數調整建議**:
- **一般場景**: CLAHE(clipLimit=3.0, tileGridSize=(8,8)) + Canny(50,150)
- **低對比度場景**: CLAHE(clipLimit=4.0, tileGridSize=(4,4)) + Canny(30,120)
- **高對比度場景**: CLAHE(clipLimit=2.0, tileGridSize=(16,16)) + Canny(80,200)
- **精密檢測場景**: CLAHE(clipLimit=2.5, tileGridSize=(8,8)) + Canny(40,120)

**Enhanced Canny 的優勢**:
- **對比度增強**: CLAHE 預處理使得邊緣檢測更加精確
- **低光適應**: 在低光環境下仍能檢測出清晰邊緣
- **噪聲抑制**: CLAHE 的對比度限制功能有助於減少噪聲干擾
- **特徵互補**: 邊緣特徵與 RGB 色彩特徵形成良好互補

通過這種循序漸進的前處理方式，Enhanced Canny 通道能夠為 YOLO 模型提供高品質的邊緣特徵，顯著提升檢測性能，特別是在邊界模糊和低對比度場景下。